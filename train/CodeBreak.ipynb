{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CodeBreak.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "foVGKp7Kyrq8",
        "colab_type": "code",
        "outputId": "c70b7919-c313-4ac8-c790-dddbb0121e72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!pip3 install face_recognition"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: face_recognition in /usr/local/lib/python3.6/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from face_recognition) (1.17.4)\n",
            "Requirement already satisfied: face-recognition-models>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (0.3.0)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (19.18.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (7.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from face_recognition) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->face_recognition) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs5CBzZvWi6r",
        "colab_type": "code",
        "outputId": "5081ea84-dadb-4882-a3f3-170c75dd292a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/antoinelame/GazeTracking/master/gaze_tracking/trained_models/shape_predictor_68_face_landmarks.dat"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-21 23:18:44--  https://raw.githubusercontent.com/antoinelame/GazeTracking/master/gaze_tracking/trained_models/shape_predictor_68_face_landmarks.dat\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99693937 (95M) [application/octet-stream]\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat.1’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  95.08M   167MB/s    in 0.6s    \n",
            "\n",
            "2019-12-21 23:18:49 (167 MB/s) - ‘shape_predictor_68_face_landmarks.dat.1’ saved [99693937/99693937]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e_0DMNV8i8p",
        "colab_type": "code",
        "outputId": "1dcdba45-2543-4da8-de7a-ab742f6794b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import face_recognition\n",
        "import keras\n",
        "from keras.models import load_model\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9Sbxgkf8q0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip freeze> requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U9r2W7PF1Q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMOTION_DICT= {'Angry': 0, 'Sad': 5, 'Neutral': 4, 'Disgust': 1, 'Surprise': 6, 'Fear': 2, 'Happy': 3}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-hPuBu08sAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_face(frame):\n",
        "    #image = face_recognition.load_image_file(img_name)\n",
        "    face_locations = face_recognition.face_locations(frame)\n",
        "\n",
        "\n",
        "    top, right, bottom, left = face_locations[0]\n",
        "    face_image1 = frame[top:bottom, left:right]\n",
        "    #plt.imshow(face_image1)\n",
        "    return(face_image1)\n",
        "    \n",
        "    #image_save = Image.fromarray(face_image1) \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DBRR1JCFCH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_emotion_model(model_name):\n",
        "    model = load_model(model_name)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZiIQs4gA1EW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def emotion_rec(model, face_image):\n",
        "    face_image = cv2.resize(face_image, (48,48))\n",
        "    face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)\n",
        "    face_image = np.reshape(face_image, [1, face_image.shape[0], face_image.shape[1], 1])\n",
        "    predicted_class = np.argmax(model.predict(face_image))\n",
        "    if(predicted_class == 6 or predicted_class == 2):\n",
        "        predicted_class = 4\n",
        "    label_map = dict((v,k) for k,v in EMOTION_DICT.items()) \n",
        "    predicted_label = label_map[predicted_class]\n",
        "    return predicted_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf-p0B21VIEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "class Pupil(object):\n",
        "    \"\"\"\n",
        "    This class detects the iris of an eye and estimates\n",
        "    the position of the pupil\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, eye_frame, threshold):\n",
        "        self.iris_frame = None\n",
        "        self.threshold = threshold\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "\n",
        "        self.detect_iris(eye_frame)\n",
        "\n",
        "    @staticmethod\n",
        "    def image_processing(eye_frame, threshold):\n",
        "        \"\"\"Performs operations on the eye frame to isolate the iris\n",
        "        Arguments:\n",
        "            eye_frame (numpy.ndarray): Frame containing an eye and nothing else\n",
        "            threshold (int): Threshold value used to binarize the eye frame\n",
        "        Returns:\n",
        "            A frame with a single element representing the iris\n",
        "        \"\"\"\n",
        "        kernel = np.ones((3, 3), np.uint8)\n",
        "        new_frame = cv2.bilateralFilter(eye_frame, 10, 15, 15)\n",
        "        new_frame = cv2.erode(new_frame, kernel, iterations=3)\n",
        "        new_frame = cv2.threshold(new_frame, threshold, 255, cv2.THRESH_BINARY)[1]\n",
        "\n",
        "        return new_frame\n",
        "\n",
        "    def detect_iris(self, eye_frame):\n",
        "        \"\"\"Detects the iris and estimates the position of the iris by\n",
        "        calculating the centroid.\n",
        "        Arguments:\n",
        "            eye_frame (numpy.ndarray): Frame containing an eye and nothing else\n",
        "        \"\"\"\n",
        "        self.iris_frame = self.image_processing(eye_frame, self.threshold)\n",
        "\n",
        "        contours, _ = cv2.findContours(self.iris_frame, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)[-2:]\n",
        "        contours = sorted(contours, key=cv2.contourArea)\n",
        "\n",
        "        try:\n",
        "            moments = cv2.moments(contours[-2])\n",
        "            self.x = int(moments['m10'] / moments['m00'])\n",
        "            self.y = int(moments['m01'] / moments['m00'])\n",
        "        except (IndexError, ZeroDivisionError):\n",
        "            pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKY8Qq_pU88N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "class Eye(object):\n",
        "    \"\"\"\n",
        "    This class creates a new frame to isolate the eye and\n",
        "    initiates the pupil detection.\n",
        "    \"\"\"\n",
        "\n",
        "    LEFT_EYE_POINTS = [36, 37, 38, 39, 40, 41]\n",
        "    RIGHT_EYE_POINTS = [42, 43, 44, 45, 46, 47]\n",
        "\n",
        "    def __init__(self, original_frame, landmarks, side, calibration):\n",
        "        self.frame = None\n",
        "        self.origin = None\n",
        "        self.center = None\n",
        "        self.pupil = None\n",
        "\n",
        "        self._analyze(original_frame, landmarks, side, calibration)\n",
        "\n",
        "    @staticmethod\n",
        "    def _middle_point(p1, p2):\n",
        "        \"\"\"Returns the middle point (x,y) between two points\n",
        "        Arguments:\n",
        "            p1 (dlib.point): First point\n",
        "            p2 (dlib.point): Second point\n",
        "        \"\"\"\n",
        "        x = int((p1.x + p2.x) / 2)\n",
        "        y = int((p1.y + p2.y) / 2)\n",
        "        return (x, y)\n",
        "\n",
        "    def _isolate(self, frame, landmarks, points):\n",
        "        \"\"\"Isolate an eye, to have a frame without other part of the face.\n",
        "        Arguments:\n",
        "            frame (numpy.ndarray): Frame containing the face\n",
        "            landmarks (dlib.full_object_detection): Facial landmarks for the face region\n",
        "            points (list): Points of an eye (from the 68 Multi-PIE landmarks)\n",
        "        \"\"\"\n",
        "        region = np.array([(landmarks.part(point).x, landmarks.part(point).y) for point in points])\n",
        "        region = region.astype(np.int32)\n",
        "\n",
        "        # Applying a mask to get only the eye\n",
        "        height, width = frame.shape[:2]\n",
        "        black_frame = np.zeros((height, width), np.uint8)\n",
        "        mask = np.full((height, width), 255, np.uint8)\n",
        "        cv2.fillPoly(mask, [region], (0, 0, 0))\n",
        "        eye = cv2.bitwise_not(black_frame, frame.copy(), mask=mask)\n",
        "\n",
        "        # Cropping on the eye\n",
        "        margin = 5\n",
        "        min_x = np.min(region[:, 0]) - margin\n",
        "        max_x = np.max(region[:, 0]) + margin\n",
        "        min_y = np.min(region[:, 1]) - margin\n",
        "        max_y = np.max(region[:, 1]) + margin\n",
        "\n",
        "        self.frame = eye[min_y:max_y, min_x:max_x]\n",
        "        self.origin = (min_x, min_y)\n",
        "\n",
        "        height, width = self.frame.shape[:2]\n",
        "        self.center = (width / 2, height / 2)\n",
        "\n",
        "    def _blinking_ratio(self, landmarks, points):\n",
        "        \"\"\"Calculates a ratio that can indicate whether an eye is closed or not.\n",
        "        It's the division of the width of the eye, by its height.\n",
        "        Arguments:\n",
        "            landmarks (dlib.full_object_detection): Facial landmarks for the face region\n",
        "            points (list): Points of an eye (from the 68 Multi-PIE landmarks)\n",
        "        Returns:\n",
        "            The computed ratio\n",
        "        \"\"\"\n",
        "        left = (landmarks.part(points[0]).x, landmarks.part(points[0]).y)\n",
        "        right = (landmarks.part(points[3]).x, landmarks.part(points[3]).y)\n",
        "        top = self._middle_point(landmarks.part(points[1]), landmarks.part(points[2]))\n",
        "        bottom = self._middle_point(landmarks.part(points[5]), landmarks.part(points[4]))\n",
        "\n",
        "        eye_width = math.hypot((left[0] - right[0]), (left[1] - right[1]))\n",
        "        eye_height = math.hypot((top[0] - bottom[0]), (top[1] - bottom[1]))\n",
        "\n",
        "        try:\n",
        "            ratio = eye_width / eye_height\n",
        "        except ZeroDivisionError:\n",
        "            ratio = None\n",
        "\n",
        "        return ratio\n",
        "\n",
        "    def _analyze(self, original_frame, landmarks, side, calibration):\n",
        "        \"\"\"Detects and isolates the eye in a new frame, sends data to the calibration\n",
        "        and initializes Pupil object.\n",
        "        Arguments:\n",
        "            original_frame (numpy.ndarray): Frame passed by the user\n",
        "            landmarks (dlib.full_object_detection): Facial landmarks for the face region\n",
        "            side: Indicates whether it's the left eye (0) or the right eye (1)\n",
        "            calibration (calibration.Calibration): Manages the binarization threshold value\n",
        "        \"\"\"\n",
        "        if side == 0:\n",
        "            points = self.LEFT_EYE_POINTS\n",
        "        elif side == 1:\n",
        "            points = self.RIGHT_EYE_POINTS\n",
        "        else:\n",
        "            return\n",
        "\n",
        "        self.blinking = self._blinking_ratio(landmarks, points)\n",
        "        self._isolate(original_frame, landmarks, points)\n",
        "\n",
        "        if not calibration.is_complete():\n",
        "            calibration.evaluate(self.frame, side)\n",
        "\n",
        "        threshold = calibration.threshold(side)\n",
        "        self.pupil = Pupil(self.frame, threshold)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqtk6SLfU0zG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "import cv2\n",
        "\n",
        "\n",
        "\n",
        "class Calibration(object):\n",
        "    \"\"\"\n",
        "    This class calibrates the pupil detection algorithm by finding the\n",
        "    best binarization threshold value for the person and the webcam.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nb_frames = 20\n",
        "        self.thresholds_left = []\n",
        "        self.thresholds_right = []\n",
        "\n",
        "    def is_complete(self):\n",
        "        \"\"\"Returns true if the calibration is completed\"\"\"\n",
        "        return len(self.thresholds_left) >= self.nb_frames and len(self.thresholds_right) >= self.nb_frames\n",
        "\n",
        "    def threshold(self, side):\n",
        "        \"\"\"Returns the threshold value for the given eye.\n",
        "        Argument:\n",
        "            side: Indicates whether it's the left eye (0) or the right eye (1)\n",
        "        \"\"\"\n",
        "        if side == 0:\n",
        "            return int(sum(self.thresholds_left) / len(self.thresholds_left))\n",
        "        elif side == 1:\n",
        "            return int(sum(self.thresholds_right) / len(self.thresholds_right))\n",
        "\n",
        "    @staticmethod\n",
        "    def iris_size(frame):\n",
        "        \"\"\"Returns the percentage of space that the iris takes up on\n",
        "        the surface of the eye.\n",
        "        Argument:\n",
        "            frame (numpy.ndarray): Binarized iris frame\n",
        "        \"\"\"\n",
        "        frame = frame[5:-5, 5:-5]\n",
        "        height, width = frame.shape[:2]\n",
        "        nb_pixels = height * width\n",
        "        nb_blacks = nb_pixels - cv2.countNonZero(frame)\n",
        "        return nb_blacks / nb_pixels\n",
        "\n",
        "    @staticmethod\n",
        "    def find_best_threshold(eye_frame):\n",
        "        \"\"\"Calculates the optimal threshold to binarize the\n",
        "        frame for the given eye.\n",
        "        Argument:\n",
        "            eye_frame (numpy.ndarray): Frame of the eye to be analyzed\n",
        "        \"\"\"\n",
        "        average_iris_size = 0.48\n",
        "        trials = {}\n",
        "\n",
        "        for threshold in range(5, 100, 5):\n",
        "            iris_frame = Pupil.image_processing(eye_frame, threshold)\n",
        "            trials[threshold] = Calibration.iris_size(iris_frame)\n",
        "\n",
        "        best_threshold, iris_size = min(trials.items(), key=(lambda p: abs(p[1] - average_iris_size)))\n",
        "        return best_threshold\n",
        "\n",
        "    def evaluate(self, eye_frame, side):\n",
        "        \"\"\"Improves calibration by taking into consideration the\n",
        "        given image.\n",
        "        Arguments:\n",
        "            eye_frame (numpy.ndarray): Frame of the eye\n",
        "            side: Indicates whether it's the left eye (0) or the right eye (1)\n",
        "        \"\"\"\n",
        "        threshold = self.find_best_threshold(eye_frame)\n",
        "\n",
        "        if side == 0:\n",
        "            self.thresholds_left.append(threshold)\n",
        "        elif side == 1:\n",
        "            self.thresholds_right.append(threshold)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2-FbECuUiQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "import os\n",
        "import cv2\n",
        "import dlib\n",
        "\n",
        "\n",
        "class GazeTracking(object):\n",
        "    \"\"\"\n",
        "    This class tracks the user's gaze.\n",
        "    It provides useful information like the position of the eyes\n",
        "    and pupils and allows to know if the eyes are open or closed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.frame = None\n",
        "        self.eye_left = None\n",
        "        self.eye_right = None\n",
        "        self.calibration = Calibration()\n",
        "\n",
        "        # _face_detector is used to detect faces\n",
        "        self._face_detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "        # _predictor is used to get facial landmarks of a given face\n",
        "        model_path = \"shape_predictor_68_face_landmarks.dat\"\n",
        "        self._predictor = dlib.shape_predictor(model_path)\n",
        "\n",
        "    @property\n",
        "    def pupils_located(self):\n",
        "        \"\"\"Check that the pupils have been located\"\"\"\n",
        "        try:\n",
        "            int(self.eye_left.pupil.x)\n",
        "            int(self.eye_left.pupil.y)\n",
        "            int(self.eye_right.pupil.x)\n",
        "            int(self.eye_right.pupil.y)\n",
        "            return True\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    def _analyze(self):\n",
        "        \"\"\"Detects the face and initialize Eye objects\"\"\"\n",
        "        frame = cv2.cvtColor(self.frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = self._face_detector(frame)\n",
        "\n",
        "        try:\n",
        "            landmarks = self._predictor(frame, faces[0])\n",
        "            self.eye_left = Eye(frame, landmarks, 0, self.calibration)\n",
        "            self.eye_right = Eye(frame, landmarks, 1, self.calibration)\n",
        "\n",
        "        except IndexError:\n",
        "            self.eye_left = None\n",
        "            self.eye_right = None\n",
        "\n",
        "    def refresh(self, frame):\n",
        "        \"\"\"Refreshes the frame and analyzes it.\n",
        "        Arguments:\n",
        "            frame (numpy.ndarray): The frame to analyze\n",
        "        \"\"\"\n",
        "        self.frame = frame\n",
        "        self._analyze()\n",
        "\n",
        "    def pupil_left_coords(self):\n",
        "        \"\"\"Returns the coordinates of the left pupil\"\"\"\n",
        "        if self.pupils_located:\n",
        "            x = self.eye_left.origin[0] + self.eye_left.pupil.x\n",
        "            y = self.eye_left.origin[1] + self.eye_left.pupil.y\n",
        "            return (x, y)\n",
        "\n",
        "    def pupil_right_coords(self):\n",
        "        \"\"\"Returns the coordinates of the right pupil\"\"\"\n",
        "        if self.pupils_located:\n",
        "            x = self.eye_right.origin[0] + self.eye_right.pupil.x\n",
        "            y = self.eye_right.origin[1] + self.eye_right.pupil.y\n",
        "            return (x, y)\n",
        "\n",
        "    def horizontal_ratio(self):\n",
        "        \"\"\"Returns a number between 0.0 and 1.0 that indicates the\n",
        "        horizontal direction of the gaze. The extreme right is 0.0,\n",
        "        the center is 0.5 and the extreme left is 1.0\n",
        "        \"\"\"\n",
        "        if self.pupils_located:\n",
        "            pupil_left = self.eye_left.pupil.x / (self.eye_left.center[0] * 2 - 10)\n",
        "            pupil_right = self.eye_right.pupil.x / (self.eye_right.center[0] * 2 - 10)\n",
        "            return (pupil_left + pupil_right) / 2\n",
        "\n",
        "    def vertical_ratio(self):\n",
        "        \"\"\"Returns a number between 0.0 and 1.0 that indicates the\n",
        "        vertical direction of the gaze. The extreme top is 0.0,\n",
        "        the center is 0.5 and the extreme bottom is 1.0\n",
        "        \"\"\"\n",
        "        if self.pupils_located:\n",
        "            pupil_left = self.eye_left.pupil.y / (self.eye_left.center[1] * 2 - 10)\n",
        "            pupil_right = self.eye_right.pupil.y / (self.eye_right.center[1] * 2 - 10)\n",
        "            return (pupil_left + pupil_right) / 2\n",
        "\n",
        "    def is_right(self):\n",
        "        \"\"\"Returns true if the user is looking to the right\"\"\"\n",
        "        if self.pupils_located:\n",
        "            return self.horizontal_ratio() <= 0.35\n",
        "\n",
        "    def is_left(self):\n",
        "        \"\"\"Returns true if the user is looking to the left\"\"\"\n",
        "        if self.pupils_located:\n",
        "            return self.horizontal_ratio() >= 0.65\n",
        "\n",
        "    def is_center(self):\n",
        "        \"\"\"Returns true if the user is looking to the center\"\"\"\n",
        "        if self.pupils_located:\n",
        "            return self.is_right() is not True and self.is_left() is not True\n",
        "\n",
        "    def is_blinking(self):\n",
        "        \"\"\"Returns true if the user closes his eyes\"\"\"\n",
        "        if self.pupils_located:\n",
        "            blinking_ratio = (self.eye_left.blinking + self.eye_right.blinking) / 2\n",
        "            return blinking_ratio > 3.8\n",
        "\n",
        "    def annotated_frame(self):\n",
        "        \"\"\"Returns the main frame with pupils highlighted\"\"\"\n",
        "        frame = self.frame.copy()\n",
        "\n",
        "        if self.pupils_located:\n",
        "            color = (0, 255, 0)\n",
        "            x_left, y_left = self.pupil_left_coords()\n",
        "            x_right, y_right = self.pupil_right_coords()\n",
        "            cv2.line(frame, (x_left - 5, y_left), (x_left + 5, y_left), color)\n",
        "            cv2.line(frame, (x_left, y_left - 5), (x_left, y_left + 5), color)\n",
        "            cv2.line(frame, (x_right - 5, y_right), (x_right + 5, y_right), color)\n",
        "            cv2.line(frame, (x_right, y_right - 5), (x_right, y_right + 5), color)\n",
        "\n",
        "        return frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW82FkiDFZGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Gaze(frame):\n",
        "    gaze = GazeTracking()\n",
        "\n",
        "    gaze.refresh(frame)\n",
        "    frame = gaze.annotated_frame()\n",
        "    pos = 0\n",
        "\n",
        "    if gaze.is_blinking():\n",
        "        pos = 0.2\n",
        "    elif gaze.is_right():\n",
        "        pos = 0.7\n",
        "    elif gaze.is_left():\n",
        "        pos = 0.7\n",
        "    elif gaze.is_center():\n",
        "        pos = 1\n",
        "    else:\n",
        "        pos = 0\n",
        "\n",
        "\n",
        "    left_pupil = gaze.pupil_left_coords()\n",
        "    right_pupil = gaze.pupil_right_coords()\n",
        "\n",
        "    return(pos)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e07Z2NOkBBuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analyse_video(subject_vid):\n",
        "    model = load_emotion_model(\"model_v6_23.hdf5\")\n",
        "    with open('innovators.csv', 'wt') as f:\n",
        "        f.write('time, attention, emotion\\n')\n",
        "\n",
        "        cap = cv2.VideoCapture(subject_vid)\n",
        "        count = 0\n",
        "        while(cap.isOpened()):\n",
        "            ret, frame = cap.read()\n",
        "            if ret:\n",
        "                nframe = frame[int(frame.shape[0]*0.05):int(frame.shape[0]*0.8), int(0.2*frame.shape[1]):int(0.8*frame.shape[1])]\n",
        "                count += 30 # i.e. at 30 fps, this advances one second\n",
        "                cap.set(1, count)\n",
        "                r1 = count/30\n",
        "                r2 = Gaze(nframe)\n",
        "                r3 = emotion_rec(model,crop_face(nframe))\n",
        "                f.write(\",\".join([str(r1), str(r2), str(r3)]))\n",
        "                print(r1, r2, r3)\n",
        "            else:\n",
        "                cap.release()\n",
        "                break\n",
        "\n",
        "            f.write(\"\\n\")\n",
        "        cap.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGHymlBAKjuT",
        "colab_type": "code",
        "outputId": "75d46738-0ca4-409a-c778-168eb8f51659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "analyse_video(\"demo1.mp4\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "240\n",
            "480\n",
            "720\n",
            "960\n",
            "1200\n",
            "1440\n",
            "1680\n",
            "1920\n",
            "2160\n",
            "2400\n",
            "2640\n",
            "2880\n",
            "3120\n",
            "3360\n",
            "3600\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}